{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this modern world recommendation systems are everywhere, whether you shop online on Amazon or watch a movie on Netflix or read an article on Medium. Recommendation systems are constantly working to bring up the items that a user is most likely to like. Simply put, recommendation system is a program that predicts the rating of a user towards/for an item.  \n",
    "\n",
    "Recommending a movie to a user is a challenging task as there is a lot more information available about the movie like the genre, actors, language etc. as well as user behaviour patterns and clicks for example whether user watched the trailer, clicked on the movie as well as ratings user gave to that movie.   \n",
    "\n",
    "## Recommendation System Techniques\n",
    "### I Content Based Filtering\n",
    "It is a technique where the recommendation is based on the data about the item. The algorithm recommends the items that are similar in characteristic to the items user liked in the past. For example, if the user like Spider man 1 movie then this recommendation system recommends Spider man 2 as it has the same genre and star cast. \n",
    "\n",
    "\n",
    "### II Collaborative Filtering\n",
    "It is an approach where recommendation is based on user's behaviour and comparing and contrasting it with other users behaviour. The history of all users plays an important role in this technique. The algorithm works by searching a large group of users data and finding a small number of users whose choice matches well with the particular user. **Collaborative Filtering** is the most popular recommendation techniques used by Amazon, Netflix and YouTube \n",
    "\n",
    "There are 2 types of collaborative filtering \n",
    "#### a. User-based  Collaborative Filtering\n",
    "The basic concept here is to find other users that have similar preference pattern for the user say A and then recommending user A items that other similar users liked but its not viewed by user A. This is done by creating a matrix of items user has rated/clicked/liked and then generating similarity score between users and then recommending items  \n",
    "For example, if user A liked Batman Begins, Justice League and Thor and user B likes Batman Begins, Justice League and Avengers then they have similar interests. We can then recommend Thor to user B and Avengers to user A.\n",
    "\n",
    "#### b. Item-based Collaborative Filtering\n",
    "This was developed by Amazon. In this approach we find similar items instead of similar user and then recommending the similar items to the user. This is done by creating a matrix of items that the user **(same user)** liked/clicked/rated and then measuring the similarity of that item across all users who rated/ viewed/ clicked both and then finally recommending them based on similarity score\n",
    "For example,If suppose a user A rated Money Heist 5 and Ozark 5 and user B watched and rated Money Heist 4 and Ozark 4 and now we have person C who watched Money heist and rated it 5.Now our recommender system  take these two shows 'Money Heist' and 'Ozark' and check the rating by all users who rated both the shows (in our case A and B). Now since A and B rated both shows same then it means that these shows might have  similarity ie. items are similar   and thus system will recommend user C 'Ozark'  \n",
    "\n",
    "Simpley said-For an item I, a set of similar items are determined based on rating vectors consisting of received user ratings. The rating by a user U, who hasnâ€™t rated it, is found by picking out N items from the similarity list that have been rated by U and calculating the rating based on these N ratings\n",
    "\n",
    "#### Pros and Cons\n",
    "1. People are ficle minded and their views change over time however items remains the same\n",
    "2. There are fewer items than users, hence, it is easy to computer item-based computing\n",
    "3. User based system can be tricked with shilling attach\n",
    "\n",
    "For this project I will use item-based collaborative filtering to buil the recommmender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in Collaborative Filtering\n",
    "To build a system that automatically recommends new movies to user based on ratings/ clicks of other users there are 2 steps involved as follows:\n",
    "1. Finding similar users  (for user based)\n",
    "2. Predict ratings of the items that are not yet rated/ viewed by the user  \n",
    "\n",
    "To do this succesfully we need to come up with methods to answer the following questions:\n",
    "1. How are we going to find similar users ?\n",
    "2. Given we find similar users, how do we predict the rating that a user will give to a particular item based on the ratings for that item by similar users ?\n",
    "3. How do we evaluate the performance of the prediction model ? \n",
    "\n",
    "There are many ways/algorithms by which this can be done. For example we can use euclidean distance or cosine distance to find the similar users and then take average of these user's rating as prediction for rating that a user would give to the movie suggested. To evaluate model we can use RMSE on the test dataset with actual rating by the user. Since not all users rate the movies they watch, the user-item matrix is usually sparse ie. mostly empty, therefore, there are various complex algoritms involved that involve steps like dimention reduction or matrix factorization (eg SVD or PCA)  \n",
    "\n",
    "ref: https://realpython.com/build-recommendation-engine-collaborative-filtering/  \n",
    "ref: http://www.mmds.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization\n",
    "In user-item matrix, there are two dimensions ie. the users and items. If the matrix is mostly empty then to boost the performance of the model we must reduce its dimensions by factorizing it. \n",
    "**Matrix Factorization** is a technique in which we break down a large matrix into a product of smaller matrices. For example, a **m x n** matrix can be broken down into 2 matices of **m x p and p x n** \n",
    "\n",
    "**Remember** A matrix A can be multiplied with matrix B only if the number of columns in A = number of rows in \n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation system using Surprise Python Library\n",
    "Surprise library is one of the most popular libriary and comes with various recommendation algorithms as well as inbuild data set -movielens100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries\n",
    "import pandas as pd\n",
    "from surprise import Dataset\n",
    "from surprise import Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "movielens = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = {\n",
    "    \"item\": [1, 2, 1, 2, 1, 2, 1, 2, 1],\n",
    "    \"user\": ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'E'],\n",
    "    \"rating\": [1, 2, 2, 4, 2.5, 4, 4.5, 5, 3],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Loads Pandas dataframe\n",
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "# Loads the builtin Movielens-100k data\n",
    "movielens = Dataset.load_builtin('ml-100k')\n",
    "#The load_builtin() method will offer to download the movielens-100k dataset if it has not already been downloaded,\n",
    "#and it will save it in the .surprise_data folder in your home directory \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 Using KNN Algoritm (Distance based similarity approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**name** defines the similarity metric to use. Options are cosine, msd, pearson, or pearson_baseline. The default is msd= mean squared difference\n",
    "\n",
    "**user_based** is a boolean that tells whether the approach will be user-based or item-based. The default is True, which means the user-based approach will be used\n",
    "\n",
    "**min_support** is the minimum number of common items needed between users to consider them for similarity. For the item-based approach, this corresponds to the minimum number of common users for two items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "0.94357224442708\n",
      "{'sim_options': {'name': 'msd', 'min_support': 3, 'user_based': False}}\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "sim_options = {\n",
    "    \"name\": [\"msd\", \"cosine\"],\n",
    "    \"min_support\": [3, 4, 5],\n",
    "    \"user_based\": [False, True],\n",
    "}\n",
    "\n",
    "param_grid = {\"sim_options\": sim_options}\n",
    "\n",
    "gs = GridSearchCV(KNNWithMeans, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "For  MovieLens 100k dataset, Centered-KNN algorithm works best if you go with **item-based** approach and use **msd** as the similarity metric with **minimum support 3**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split and the fit() method\n",
    "If you donâ€™t want to run a full cross-validation procedure, you can use the train_test_split() to sample a trainset and a testset with given sizes, and use the accuracy metric of your chosing. Youâ€™ll need to use the fit() method which will train the algorithm on the trainset, and the test() method which will return the predictions made from the testset\n",
    "ref:https://surprise.readthedocs.io/en/stable/getting_started.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9421139885167851"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "# sample random trainset and testset\n",
    "# test set is made of 25% of the ratings.\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 - Using SVD Algorithm - Matrix factorization approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_epochs** is the number of iterations of SGD, which is basically an iterative method used in statistics to minimize a function  \n",
    "\n",
    "**lr_all** is the learning rate for all parameters, which is a parameter that decides how much the parameters are adjusted in each iteration  \n",
    "\n",
    "**reg_all** is the regularization term for all parameters, which is a penalty term added to prevent overfitting  \n",
    "\n",
    "**Note** Keep in mind that there wonâ€™t be any similarity metrics in matrix factorization algorithms as the latent factors take care of similarity among users or items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9635502293746322\n",
      "{'n_epochs': 10, 'lr_all': 0.005, 'reg_all': 0.4}\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_epochs\": [5, 10],\n",
    "    \"lr_all\": [0.002, 0.005],\n",
    "    \"reg_all\": [0.4, 0.6]\n",
    "}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "So, for the MovieLens 100k dataset, the SVD algorithm works best if you go with **10 epochs** and use a **learning rate of 0.005** and **0.4 regularization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions on custome dataframe using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = {\n",
    "    \"item\": [1, 2, 1, 2, 1, 2, 1, 2, 1],\n",
    "    \"user\": ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'E'],\n",
    "    \"rating\": [1, 2, 2, 4, 2.5, 4, 4.5, 5, 3],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Loads Pandas dataframe\n",
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "# Loads the builtin Movielens-100k data\n",
    "movielens = Dataset.load_builtin('ml-100k')\n",
    "#The load_builtin() method will offer to download the movielens-100k dataset if it has not already been downloaded,\n",
    "#and it will save it in the .surprise_data folder in your home directory \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "\n",
    "# To use item-based cosine similarity\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False,  # Compute  similarities between items\n",
    "}\n",
    "algo = KNNWithMeans(sim_options=sim_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.15"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSet = data.build_full_trainset()\n",
    "algo.fit(trainingSet)\n",
    "prediction = algo.predict('E', 2)\n",
    "prediction.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = {\n",
    "    \"item\": [1, 2, 1, 2, 1, 2, 1, 2, 1],\n",
    "    \"user\": ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'E'],\n",
    "    \"rating\": [1, 2, 2, 4, 2.5, 4, 4.5, 5, 3],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# Loads Pandas dataframe\n",
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "# Loads the builtin Movielens-100k data\n",
    "movielens = Dataset.load_builtin('ml-100k')\n",
    "#The load_builtin() method will offer to download the movielens-100k dataset if it has not already been downloaded,\n",
    "#and it will save it in the .surprise_data folder in your home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.52986"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "algo = SVD(n_epochs= 10,lr_all=  0.005,\n",
    "    reg_all= 0.4)\n",
    "trainingSet = data.build_full_trainset()\n",
    "algo.fit(trainingSet)\n",
    "#trainingSet = data.build_full_trainset()\n",
    "\n",
    "#algo.fit(trainingSet)\n",
    "\n",
    "prediction = algo.predict('E', 540)\n",
    "prediction.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Whole dataset  and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x7fa750564bd0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "\n",
    "#gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "algo = SVD(n_epochs= 10,lr_all=  0.005,\n",
    "    reg_all= 0.4)\n",
    "#trainset, testset = train_test_split(data, test_size=.25) # To split \n",
    "trainset = data.build_full_trainset() # Training on whole dataset\n",
    "algo.fit(trainset)\n",
    "#print(gs.best_score[\"rmse\"])\n",
    "#print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The predict() uses raw ids (please read this about raw and inner ids). \n",
    "#As the dataset we have used has been read from a file, the raw ids are strings (even if they represent numbers).\n",
    "uid = str(196)  # raw user id (as in the ratings file). They are **strings**!\n",
    "iid = str(302)  # raw item id (as in the ratings file). They are **strings**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 196        item: 302        r_ui = None   est = 4.02   {'was_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Prediction(uid='196', iid='302', r_ui=None, est=4.015285886856125, details={'was_impossible': False})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo.predict(uid, iid, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.291272980108429"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prediction = algo.predict(uid=str(20),iid=str(120))\n",
    "prediction.est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://surprise.readthedocs.io/en/stable/getting_started.html\n",
    "2. https://realpython.com/build-recommendation-engine-collaborative-filtering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
